name: CI with Ollama

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  OLLAMA_MODEL: tinyllama  # ~600MB

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create test environment
      run: |
        echo "CHAT_PROVIDER=ollama" > .env
        echo "CHAT_MODEL=${{ env.OLLAMA_MODEL }}" >> .env
        echo "OLLAMA_HOST=http://localhost:11434" >> .env

    - name: Cache Ollama models
      uses: actions/cache@v4
      id: ollama-cache
      with:
        path: ~/.ollama
        key: ollama-${{ env.OLLAMA_MODEL }}-v1-${{ runner.os }}
        restore-keys: |
          ollama-${{ env.OLLAMA_MODEL }}-v1-
          ollama-${{ env.OLLAMA_MODEL }}-
          ollama-

    - name: Show cache status
      run: |
        if [ -d ~/.ollama/models ] && [ "$(ls -A ~/.ollama/models/ 2>/dev/null)" ]; then
          echo "âœ… Using cached ${{ env.OLLAMA_MODEL }} model (~600MB saved!)"
          MODEL_SIZE=$(du -sh ~/.ollama/ 2>/dev/null | cut -f1 || echo "unknown")
          echo "ðŸ’¾ Cached Ollama data size: $MODEL_SIZE"
          echo "âš¡ CI will be ~2-3 minutes faster!"
        else
          echo "ðŸ“¥ Downloading ${{ env.OLLAMA_MODEL }} model (~600MB)"
          echo "ðŸŒ First run will be slower, but subsequent runs will be cached"
        fi

    - name: Install Ollama
      uses: ai-action/setup-ollama@v1

    - name: Pull model
      run: |
        echo "Checking if ${{ env.OLLAMA_MODEL }} model is already available..."
        if ollama list | grep -q "${{ env.OLLAMA_MODEL }}"; then
          echo "âœ… Model ${{ env.OLLAMA_MODEL }} already available, skipping pull"
        else
          echo "ðŸ“¥ Pulling ${{ env.OLLAMA_MODEL }} model..."
          ollama pull ${{ env.OLLAMA_MODEL }}
        fi

    - name: Load model
      run: |
        echo "Loading ${{ env.OLLAMA_MODEL }} model..."
        timeout 30 ollama run ${{ env.OLLAMA_MODEL }} "hello" > /dev/null 2>&1 || echo "Model load attempt complete"

    - name: Verify Ollama setup
      run: |
        echo "Checking Ollama status..."
        ps aux | grep ollama || echo "Ollama process not found"
        curl -v http://localhost:11434/api/tags 2>&1 | head -10
        curl -s http://localhost:11434/api/tags | jq '.models[0].name' 2>/dev/null || echo "Failed to get model list"

    - name: Wait for Ollama to be ready
      run: |
        echo "Waiting for Ollama to be fully ready..."
        for i in {1..30}; do
          if curl -s http://localhost:11434/api/generate -X POST -H "Content-Type: application/json" -d '{"model": "tinyllama", "prompt": "test", "stream": false}' | jq -r '.response' > /dev/null 2>&1; then
            echo "âœ… Ollama is ready!"
            break
          fi
          echo "Waiting for Ollama... ($i/30)"
          sleep 3
        done

    - name: Run Ollama client tests
      run: |
        echo "Running Ollama integration tests..."
        python test_ollama.py

    - name: Run pytest tests
      run: |
        python -m pytest -v --tb=short -m "not integration"

    - name: Run integration tests (with Ollama)
      run: |
        python -m pytest -v --tb=short -m integration